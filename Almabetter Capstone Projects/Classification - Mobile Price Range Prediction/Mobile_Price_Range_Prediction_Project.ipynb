{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "T5CmagL3EC8N",
        "TIqpNgepFxVj",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Mobile Price Range Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Dipak Someshwar"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now a days, a cellphone is an essential accessory of a person. It is the fastest evolving and moving product in the technology market space. New mobiles with updated versions and new features are introduced into the market at a rapid pace. Thousands of mobiles are sold each day. In such a fast-paced and volatile market, a mobile company needs to set optimal prices to complete with its rivals.This kind of prediction will help companies estimate price of mobiles to give tough competion to other mobile manufacturer.Also it will be usefull for Consumers to verify that they are paying best price for a mobile.\n",
        "\n",
        "In this project, we are going to explore and analyze a dataset which contains specifications of two thousand mobile phones and try to predict optimum price ranges for a list of mobile phones in the market.During the purchase of mobile phones, various features like memory, display, battery, camera, etc., are considered. People fail to make correct decisions, due to the non-availability of necessary resources to cross-validate the price. To address this issue, a machine learning model is developed using the data related to the key features of the mobile phone. The developed model is then used to predict the price range of the new mobile phone.use the machine learning algorithms namely Support Vector Machine (SVM), Random Forest Classifier (RFC), Logistic Regression,Decission Tree,Naive Bayes Theorm,K-nearest neighbors are used to train the model and predict the output as low, medium, high or very high of price range."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In the competitive mobile phone market companies want to understand sales data of mobile phones and factors which drive the prices. The objective is to find out some relation between features of a mobile phoneleg - RAM Internal Memory, etc) and its selling price in this problem, we do not have to predict the actual price but a price range indicating how high the price is ?**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy import stats\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's mount the google drive for import the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-gVjG_XR1ATa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "mobile_df = pd.read_csv('/content/drive/MyDrive/AlmaBetter/data_mobile_price_range.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "mobile_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(mobile_df.shape)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "mobile_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate or Non Duplicate Value Count\n",
        "mobile_df.duplicated().value_counts()"
      ],
      "metadata": {
        "id": "v15gRHFq3EQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "len(mobile_df[mobile_df.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(mobile_df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(mobile_df.isnull())"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.The given dataset from competitive mobile market companies, and we do not have to predict the actual price but a price range indicating how high the price is.\n",
        "\n",
        "2.Mobile phones come in all sorts of prices, features, specifications and all. Price estimation and prediction is an important part of consumer strategy. Deciding on the correct price of a product is very important for the market success of a product. A new product that has to be launched, must have the correct price so that consumers find it appropriate to buy the product.\n",
        "\n",
        "3.The above dataset has 2000 rows and 21 columns. There are no mising values and duplicate values in the dataset."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "mobile_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "mobile_df.describe().T.style.background_gradient()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Battery_power** : Total energy a battery can store in one time measured in mAh\n",
        "\n",
        "**Blue** : Has bluetooth or not\n",
        "\n",
        "**Clock_speed** : speed at which microprocessor executes instructions\n",
        "\n",
        "**Dual_sim** : Has dual sim support or not\n",
        "\n",
        "**Fc** : Front Camera mega pixels\n",
        "\n",
        "**Four_g** : Has 4G or not\n",
        "\n",
        "**Int_memory** : Internal Memory in Gigabytes\n",
        "\n",
        "**M_dep** : Mobile Depth in cm\n",
        "\n",
        "**Mobile_wt** : Weight of mobile phone\n",
        "\n",
        "**N_cores** : Number of cores of processor\n",
        "\n",
        "**Pc** : Primary Camera mega pixels\n",
        "\n",
        "**Px_height** : Pixel Resolution Height\n",
        "\n",
        "**Px_width** : Pixel Resolution Width\n",
        "\n",
        "**Ram** : Random Access Memory in Mega\n",
        "\n",
        "**Touch_screen** : Has touch screen or not\n",
        "\n",
        "**Wifi** : Has wifi or not\n",
        "\n",
        "**Sc_h** : Screen Height of mobile in cm\n",
        "\n",
        "**Sc_w** : Screen Width of mobile in cm\n",
        "\n",
        "**Talk_time** : longest time that a single battery charge will last when you are\n",
        "\n",
        "**Three_g** : Has 3G or not\n",
        "\n",
        "**Wifi** : Has wifi or not\n",
        "\n",
        "**Price_range** : This is the target variable with value of 0(low cost), (medium cost),2(high cost) and 3(very high cost)."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in mobile_df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",mobile_df[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for column in mobile_df.columns:\n",
        "  print(str(column) + ' : ' + str(mobile_df[column].unique()))\n",
        "  print('____________________________________________')"
      ],
      "metadata": {
        "id": "Dt_76iEV6DF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "# Create a copy of the current dataset and assigning to df\n",
        "df = mobile_df.copy()\n",
        "\n",
        "# Checking mobile belongs to price range 0\n",
        "print(\"No of mobile belongs to price_range 0 is :\",len(df[df['price_range']==0]))\n",
        "\n",
        "# Checking mobile belongs to price range 1\n",
        "print(\"No of mobile belongs to price_range 1 is :\",len(df[df['price_range']==1]))\n",
        "\n",
        "# Checking mobile belongs to price range 2\n",
        "print(\"No of mobile belongs to price_range 2 is :\",len(df[df['price_range']==2]))\n",
        "\n",
        "# Checking mobile belongs to price range 3\n",
        "print(\"No of mobile belongs to price_range 3 is :\",len(df[df['price_range']==3]))"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show value counts\n",
        "df['price_range'].value_counts()"
      ],
      "metadata": {
        "id": "NCZMVJkz629P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mobile phones groupby 4g support and not supports\n",
        "df.groupby('four_g')['price_range'].value_counts().reset_index(name=\"Count\").sort_values([\"Count\"],ascending=False)"
      ],
      "metadata": {
        "id": "AgBby6uw8ahh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mobile phones groupby 3g support and not supports\n",
        "df.groupby('three_g')['price_range'].value_counts().reset_index(name=\"Count\").sort_values([\"Count\"],ascending=False)"
      ],
      "metadata": {
        "id": "0T_iz1Kj9ZFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mobile phones groupby blue support and not supports\n",
        "df.groupby('blue')['price_range'].value_counts().reset_index(name=\"Count\").sort_values([\"Count\"],ascending=False)"
      ],
      "metadata": {
        "id": "RlzkYsBRAS7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mobile phones groupby dual sim support and not supports\n",
        "df.groupby('dual_sim')['price_range'].value_counts().reset_index(name=\"Count\").sort_values([\"Count\"],ascending=False)"
      ],
      "metadata": {
        "id": "tu08XVTRAXuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mobile phones groupby touch screen support and not supports\n",
        "df.groupby('touch_screen')['price_range'].value_counts().reset_index(name=\"Count\").sort_values([\"Count\"],ascending=False)"
      ],
      "metadata": {
        "id": "YElZ79RWAc8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pixels count\n",
        "df['px_count'] = df['px_width']*df['px_height']"
      ],
      "metadata": {
        "id": "sXkeqwnCAvBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show price range with descreasing pixel count values\n",
        "df.sort_values([\"px_count\"],ascending=False)[ ['price_range', 'px_count'] ].head(10)"
      ],
      "metadata": {
        "id": "2Y8iCEbWCGyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "data_num = mobile_df[['battery_power',  'clock_speed' , 'fc','int_memory','m_dep', 'mobile_wt','n_cores', 'pc',\n",
        "                      'px_height','px_width','ram', 'sc_h', 'sc_w', 'talk_time']]\n",
        "\n",
        "data_cat = mobile_df[['blue','dual_sim', 'four_g','three_g','touch_screen', 'wifi']]"
      ],
      "metadata": {
        "id": "kp3TZ2QpCQJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_num"
      ],
      "metadata": {
        "id": "2JyNc815C3Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_cat"
      ],
      "metadata": {
        "id": "5DG-0svsC-xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **The dependent variable 4 types of unique variable and all 4 same length of value are their.**"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Dependent Column Value Counts\n",
        "print(mobile_df.price_range.value_counts())\n",
        "print(\" \")\n",
        "# Dependent Variable Column Visualization\n",
        "mobile_df['price_range'].value_counts().plot(kind='pie',\n",
        "                              figsize=(5,8),\n",
        "                              autopct=\"%1.1f%%\",\n",
        "                               startangle=90,\n",
        "                               shadow=True,\n",
        "                               labels=[\"low cost(0)\", \"medium cost(1)\", \"high cost(2)\", \"very high cost(3)\"],\n",
        "                               colors=['blue','red','green','cyan'],\n",
        "                               explode=[0,0,0,0]\n",
        "                              )\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart expresses a part-to-whole relationship in your data. It's easy to explain the percentage comparison through area covered in a circle with different colors. Where differenet percentage comparison comes into action pie chart is used frequently. So, I used Pie chart and which helped me to get the percentage comparision of the dependant variable."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above pie chart total 2000 dataset.I got to know that, there are 500 mobile data which are low cost which is 25%, 500 mobile data which are medium cost which is 25%, 500 mobile data which are high cost which is 25% and 500 mobile data which are very high cost which is 25% of the whole mobile data given in the dataset.\n",
        "\n",
        "In other words we can say that all mobile price range category are equally important and we have to focus on all price range people to maximizes our sell and gain more profit."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mobile price prediction is depends upon many factor's like processor, ram, internal memory, camera mega pixels as much as these features increase the value of a mobile price is increased.\n",
        "\n",
        "When Xiaomi MI came into market it provides better value for the money for mobile phones. It gives more features in less price that's why it more populer than other companies. This is the company that bring new premium features in less price like IR Blaster, Finger Print Censors in very low price.\n",
        "\n",
        "To maximize the sell of mobile we need to bring more features in less prices."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "sns.countplot(x = mobile_df['price_range'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mobile_df['price_range'].value_counts()"
      ],
      "metadata": {
        "id": "5fkHpHriFdCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The countplot charts show the frequency counts of values for the different levels of a categorical or nominal variable. Sometimes, bar charts show other statistics, such as percentages.\n",
        "\n",
        "To show the count of all price range I have used bar chart and four price ranges 500 counts are there."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above count plot chart total 2000 dataset.I got to know that, there are 500 mobile data which are low cost, 500 mobile data which are medium cost, 500 mobile data which are high cost and 500 mobile data which are very high cost of the whole mobile data given in the dataset.\n",
        "\n",
        "In other words we can say that all mobile price range category are equally important and we have to focus on all price range people to maximizes our sell and gain more profit."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mobile price prediction is depends upon many factor's like processor, ram, internal memory, camera mega pixels as much as these features increase the value of a mobile price is increased.\n",
        "\n",
        "When Xiaomi MI came into market it provides better value for the money for mobile phones. It gives more features in less price that's why it more populer than other companies. This is the company that bring new premium features in less price like IR Blaster, Finger Print Censors in very low price.\n",
        "\n",
        "To maximize the sell of mobile we need to bring more features in less prices."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(15,20))\n",
        "graph = 1\n",
        "\n",
        "for i in data_num:\n",
        "    if graph <= 14:\n",
        "        plt.subplot(5,3,graph)\n",
        "        ax=sns.histplot(data_num[i], color='skyblue')\n",
        "        plt.xlabel(i,fontsize=10)\n",
        "    graph+=1\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histograms are used to visualize the distribution of a dataset. They provide insights into the frequency or count of values falling within specific intervals, known as bins."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(15,20))\n",
        "graph = 1\n",
        "\n",
        "for i in data_cat:\n",
        "    if graph <= 6:\n",
        "        plt.subplot(3,3,graph)\n",
        "        ax=sns.countplot(x = data_cat[i], palette='rainbow')\n",
        "        plt.xlabel(i,fontsize=10)\n",
        "    graph+=1\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in data_cat:\n",
        "    print(pd.pivot_table(mobile_df,index='price_range',columns=i, values='ram'))\n",
        "    print(\"==\"*20)"
      ],
      "metadata": {
        "id": "i2oztDaIoXwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The countplot charts show the frequency counts of values for the different levels of a categorical or nominal variable. Sometimes, bar charts show other statistics, such as percentages."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above count plot which shows the count of 3g, 4g, touchscreen and wifi etc. support and not support.\n",
        "\n",
        "* The usage of 3g and 4g phone is high compare to 2g or simple phone.\n",
        "* The usage of wifi support phone is little high.\n",
        "* The usage of dual sim support is little high.\n",
        "* The usage of touchscreen and keypad phone is equal."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from features such as dual_sim, 4g, 3g, touch_screen, wifi, and blue can help businesses make informed decisions, tailor their product offerings, and potentially create a positive business impact in the mobile device industry. By understanding customer preferences and incorporating desirable features, businesses can attract a wider customer base, enhance user experience, and drive sales growth."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Dealing with outliers values\n",
        "plt.figure(figsize=(15,20))\n",
        "graph = 1\n",
        "\n",
        "for i in data_num:\n",
        "    if graph <= 14:\n",
        "        plt.subplot(5,3,graph)\n",
        "        ax=sns.boxplot(data_num[i], color='green')\n",
        "        plt.xlabel(i,fontsize=10)\n",
        "    graph+=1\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Box plots are used to show distributions of numeric data values, especially when you want to compare them between multiple groups. They are built to provide high-level information at a glance, offering general information about a group of data's symmetry, skew, variance, and outliers.\n",
        "\n",
        "Thus, for each numerical varibale in the given dataset, I used box plot to analyse the outliers and interquartile range including mean, median, maximum and minimum value."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Almost all columns are symmetric distributed and mean is nearly same with median for numerical columns. Here Area code will be treated as text values as there are only 3 values in the particular column."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Box plot cannot define business impact. It's done just to see the distribution of the column data over the dataset."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binary_features = [ 'four_g', 'three_g']"
      ],
      "metadata": {
        "id": "3j2ZEcVp2Jjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Plot of binary features against price range\n",
        "\n",
        "for col in binary_features:\n",
        "  fig, (ax1, ax2) = plt.subplots(ncols = 2, figsize = (15, 6))\n",
        "\n",
        "  mobile_df[col].value_counts().plot.pie (autopct='%1.1f%%', ax = ax1, shadow=True, labeldistance=None)\n",
        "  ax1.set_title('Distribution by price range')\n",
        "  ax1.legend(['Support', 'Does not Support'])\n",
        "\n",
        "  sns.countplot(x = col, hue = 'price_range', data = mobile_df, ax = ax2, palette='Set3')\n",
        "  ax2.set_title('Distribution by price range')\n",
        "  ax2.set_xlabel(col)\n",
        "  ax2.legend(['Low Cost', 'Medium Cost', 'High Cost', 'Very High Cost'])\n",
        "  ax2.set_xticklabels(['Does not Support', 'Support'])"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pie chart is a type of graph in which a circle is divided into sectors that each represents a proportion of the whole. Pie charts are a useful way to organize data in order to see the size of components relative to four_g and three_g variable.\n",
        "\n",
        "Thus, I have used pie chart to show the percentage of having which one support and which one not support.\n",
        "\n",
        "The bar graph is used to compare the items between different groups of price range. Bar graphs are used to measure the 4 variable counting the which mobile support and which mobile are not support."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "three_g\n",
        "\n",
        "1.support=76.2%\n",
        "\n",
        "2.dosenot support=23.8\n",
        "\n",
        "++++++++++++++++++++++++++++++++++++++++++\n",
        "\n",
        "four_g\n",
        "\n",
        "1.support=52.1%\n",
        "\n",
        "2.doesnot support=47.9%\n",
        "\n",
        "+++++++++++++++++++++++++++++++++++++++++"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights about the distribution of devices supporting and not supporting 3G and 4G can help businesses make informed decisions. By focusing on the majority's preferences for 3G and 4G support, businesses can create positive impact by aligning their product offerings with market demand. However, it's crucial to monitor market trends and adapt strategies to avoid negative growth if customer preferences shift towards alternative network technologies."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "sns.boxplot(x=\"price_range\", y=\"battery_power\", data=mobile_df)"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Box plots are used to show distributions of numeric data values, especially when you want to compare them between multiple groups. They are built to provide high-level information at a glance, offering general information about a group of data's symmetry, skew, variance, and outliers.\n",
        "\n",
        "Thus, for each numerical varibale in the given dataset, I used box plot to analyse the outliers and interquartile range including mean, median, maximum and minimum value."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "plt.figure(figsize=(8,5))\n",
        "mobile_df['fc'].hist(alpha=0.5,color='green',label='Front camera')\n",
        "mobile_df['pc'].hist(alpha=0.5,color='red',label='Primary camera')\n",
        "plt.legend()\n",
        "plt.xlabel('MegaPixels')"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histograms provide a visual representation of data distributions, aiding in data exploration, understanding patterns, identifying outliers, and making informed decisions in data analysis and modeling. They serve as a fundamental tool in descriptive statistics and exploratory data analysis."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "plt.figure(1)\n",
        "plt.subplot(121)\n",
        "sns.countplot(x = 'four_g',hue= 'price_range',data  = mobile_df)\n",
        "plt.legend()\n",
        "plt.subplot(122)\n",
        "sns.countplot(x = 'three_g',hue= 'price_range',data  = mobile_df)\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The count plot is used to compare the items between different groups of price range. Count plot are used to measure the 4 variable counting the which mobile support 3g, 4g and which mobile are not support 3g, 4g."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 3g and 4g support ratio is higher in all the range of phones then 3g, 4g not support phones."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights about the distribution of devices supporting and not supporting 3G and 4G can help businesses make informed decisions. By focusing on the majority's preferences for 3G and 4G support, businesses can create positive impact by aligning their product offerings with market demand. However, it's crucial to monitor market trends and adapt strategies to avoid negative growth if customer preferences shift towards alternative network technologies."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "# Visualizing code of hist plot for each columns to know the data distibution\n",
        "for col in mobile_df.describe().columns:\n",
        "  fig=plt.figure(figsize=(8,5))\n",
        "  ax=fig.gca()\n",
        "  feature= (mobile_df[col])\n",
        "  sns.distplot(mobile_df[col])\n",
        "  ax.axvline(feature.mean(),color='magenta', linestyle='dashed', linewidth=2)\n",
        "  ax.axvline(feature.median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "  ax.set_title(col)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this graph the dataset have all column check the dencity, mean and median and the mean colour is magenta and median colour is cyan."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the dataset most of the columns are normal distributed."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def outlinefree(dataCol):\n",
        "\n",
        "    sorted(dataCol)                          # sort column\n",
        "    Q1,Q3 = np.percentile(dataCol,[25,75])   # getting 25% and 75% percentile\n",
        "    IQR = Q3-Q1                              # getting IQR\n",
        "    LowerRange = Q1-(1.5 * IQR)              # getting Lowrange\n",
        "    UpperRange = Q3+(1.5 * IQR)              # getting Upperrange\n",
        "\n",
        "    colname = dataCol.tolist()               # convert column into list\n",
        "    newlist =[]                              # empty list for store new values\n",
        "    for i in range(len(colname)):\n",
        "\n",
        "        if colname[i] > UpperRange:          # list number > Upperrange\n",
        "            colname[i] = UpperRange          # then number = Upperrange\n",
        "            newlist.append(colname[i])       # append value to empty list\n",
        "        elif colname[i] < LowerRange:        # list number < Lowrange\n",
        "            colname[i] = LowerRange          # then number = Lowrange\n",
        "            newlist.append(colname[i])       # append value to empty list\n",
        "        else:\n",
        "            colname[i]                       # list number\n",
        "            newlist.append(colname[i])       # append value to empty list\n",
        "\n",
        "\n",
        "\n",
        "    return newlist"
      ],
      "metadata": {
        "id": "Kt7W3FB7ykZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(data_num.columns)):\n",
        "    new_list =  outlinefree(mobile_df.loc[:,data_num.columns[i]]) # retrun new list\n",
        "    mobile_df.loc[:,data_num.columns[i]] = new_list"
      ],
      "metadata": {
        "id": "1xCFYt9Bzw-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_final_num = mobile_df[['battery_power',  'clock_speed' , 'fc','int_memory','m_dep', 'mobile_wt','n_cores', 'pc',\n",
        "                      'px_height','px_width','ram', 'sc_h', 'sc_w', 'talk_time']]"
      ],
      "metadata": {
        "id": "cJ__b0I40BO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 Visualization code\n",
        "# After dealing with outliers values\n",
        "plt.figure(figsize=(15,20))\n",
        "graph = 1\n",
        "\n",
        "for i in data_final_num:\n",
        "    if graph <= 14:\n",
        "        plt.subplot(5,3,graph)\n",
        "        ax=sns.boxplot(data_final_num[i], color='orange')\n",
        "        plt.xlabel(i,fontsize=10)\n",
        "    graph+=1\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MKQt1-Pp0Pbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "zbCeFA4X1Tzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Box plots are used to show distributions of numeric data values, especially when you want to compare them between multiple groups. They are built to provide high-level information at a glance, offering general information about a group of data's symmetry, skew, variance, and outliers.\n",
        "\n",
        "Thus, for each numerical varibale in the given dataset, I used box plot to analyse the outliers and interquartile range including mean, median, maximum and minimum value."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that from above graph that there are no ouliers present after removing outliers."
      ],
      "metadata": {
        "id": "O8Mzp2bC1T0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(20,12))\n",
        "sns.heatmap(mobile_df.corr(),annot=True,cmap=plt.cm.Accent_r)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses. The range of correlation is [-1,1].\n",
        "\n",
        "Thus to know the correlation between all the variables along with the correlation coefficients, i used correlation heatmap."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above correlation heatmap,\n",
        "\n",
        "1.we can see total price range are positiveliy highly correlated with a value of 1.\n",
        "\n",
        "2.we can see total ram are correlated with a value of 92%.\n",
        "\n",
        "3.we can see total pc and fc are correlated with a value of 65%.\n",
        "\n",
        "price range is positively correlated only with all features and negative correlated with rest variables.\n",
        "\n",
        "Rest all correlation can be depicted from the above chart."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(mobile_df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pair plot is used to understand the best set of features to explain a relationship between two variables or to form the most separated clusters. It also helps to form some simple classification models by drawing some simple lines or make linear separation in our data-set.\n",
        "\n",
        "Thus, I used pair plot to analyse the patterns of data and realationship between the features. It's exactly same as the correlation map but here you will get the graphical representation."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart I got to know, there are less linear relationship between variables and data points aren't linearly separable.Mobile price range is clusetered. Some of the above features are linearly dependent on price range like ram, internal memory, processors and so on."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ct = pd.crosstab(mobile_df['int_memory'],mobile_df['price_range'])\n",
        "from scipy.stats import chi2_contingency\n",
        "stat,pvalue,dof,expected_R = chi2_contingency(ct)\n",
        "print(\"pvalue : \",pvalue)\n",
        "\n",
        "if pvalue <= 0.1:\n",
        "    print(\"Alternate Hypothesis passed. int_memory and price_range have Relationship\")\n",
        "else:\n",
        "    print(\"Null hypothesis passed. int_memory and price_range doesnot have  Relationship\")"
      ],
      "metadata": {
        "id": "TWiYhfN_6Ing"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chi-test: It is help to figure-out relation between features and label with \"pvalue <= 0.1\""
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Missing Values/Null Values Count\n",
        "print(mobile_df.isnull().sum())\n",
        "\n",
        "# Visualizing the missing values\n",
        "# Checking Null Value by plotting Heatmap\n",
        "sns.heatmap(mobile_df.isnull(), cbar=False)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There are no missing values to handle in the given dataset.**"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "# Outlier treatment done in above visualization part."
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used IQR measure to treat outliers.\n",
        "\n",
        "The interquartile range (IQR) is a statistical measure used to identify and handle outliers in a dataset.\n",
        "\n",
        "It is defined as the range between the first quartile (Q1) and the third quartile (Q3)."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The categorical columns are already encoded in the dataset.**"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)\n",
        "\n",
        "**There are no text columns in the given dataset which I am working on. So, Skipping this part.**"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "# Defining new variable for pixels\n",
        "mobile_df['pixels'] = data_num['px_height']*data_num['px_width']\n",
        "\n",
        "# Dropping px_height and px_width\n",
        "mobile_df.drop(['px_height', 'px_width'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# In Data Transformation there are no missing values in the dataset don't need to tranformed data."
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining X and y\n",
        "x = mobile_df.drop(['price_range'], axis = 1).values\n",
        "y = mobile_df['price_range'].values"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "kGmhHs1397lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "scaler = MinMaxScaler()\n",
        "x = scaler.fit_transform(x)"
      ],
      "metadata": {
        "id": "Rsi8swpP-B-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used MinMaxScaler technique for data scaling.\n",
        "\n",
        "MinMaxScaler is a common feature scaling technique used in data preprocessing. It is used to transform numerical features by scaling them to a specified range, typically between 0 and 1."
      ],
      "metadata": {
        "id": "8HJ8TVkeOhMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, dimensionality reduction technique is not needed here because in this dataset there is not much columns and columns are less in the dataset."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# Separate the dataset in two type one trainingis 75% of data and other testing is 25% of data\n",
        "x_train, x_test, y_train, y_test= train_test_split(x,y, test_size= 0.25, random_state=167)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have split the data in 75-25 % ratio in train-test.\n",
        "\n",
        "The 75-25 ratio for train-test split is a commonly used practice in machine learning and data analysis, although it is not a hard rule and can vary depending on the specific problem and dataset.\n",
        "\n",
        "The 75% of the data is typically allocated to the training set, while the remaining 25% is allocated to the test set."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In the target variable all class have equal records so, the dataset is not imbalanced**"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - Logistic Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying logistic regression\n",
        "logmodel= LogisticRegression()\n",
        "logmodel.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the coefficients\n",
        "logmodel.coef_"
      ],
      "metadata": {
        "id": "qeZAQ1NInZ5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the intercept value\n",
        "logmodel.intercept_"
      ],
      "metadata": {
        "id": "AECkpSRbngMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "# Get the predicted probabilities\n",
        "trainscore =  logmodel.score(x_train,y_train)\n",
        "testscore =  logmodel.score(x_test,y_test)\n",
        "\n",
        "print(\"train score: {}\".format(trainscore),'\\n')\n",
        "print(\"test score: {}\".format(testscore),'\\n')\n",
        "\n",
        "y_predlogi =  logmodel.predict(x_test)\n",
        "print(' f1 score: ',f1_score(y_test, y_predlogi,average='micro'),'\\n')\n",
        "\n",
        "# Get the confusion matrix for both train and test\n",
        "print(confusion_matrix(y_test, y_predlogi))"
      ],
      "metadata": {
        "id": "u3sFDgkrnmoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(' precision score: ',precision_score(y_test, y_predlogi,average='micro'),'\\n')\n",
        "print(' recall score: ',recall_score(y_test, y_predlogi,average='micro'),'\\n')\n",
        "print(classification_report(y_test, y_predlogi))"
      ],
      "metadata": {
        "id": "ZjJtn88ZoMv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------- LogisticRegression -------------------------------------\n",
        "probabilityValues = logmodel.predict_proba(x)\n",
        "auc = roc_auc_score(y,probabilityValues,multi_class ='ovr')\n",
        "print(auc)"
      ],
      "metadata": {
        "id": "QFgPIFhdoPPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.I have used Logistic regression to create the model. As I got not so good result.so next tryting to improving the score by using hyperparameter tuning technique.\n",
        "\n",
        "2.For training score is 91% and testing score 90%\n",
        "\n",
        "3.For testing dataset, i found precision of 90% and recall of 90% and f1-score of 90% . BUt, I am also interested to see the result for mobile price range result as\n",
        "\n",
        "a.I got low(0) price precision 96% of and recall of 94% and f1-score of 95%.\n",
        "\n",
        "b.I got medium(1) price precision 86% of and recall of 90% and f1-score of 88%.\n",
        "\n",
        "c.I got High(2) price precision 88% of and recall of 84% and f1-score of 86%.\n",
        "\n",
        "d.I got very high(3) price precision 92% of and recall of 94% and f1-score of 93%."
      ],
      "metadata": {
        "id": "y6EWvOmkPZx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques\n",
        "param = {'C':[0.01,0.1,1,10,100,110,120,130]}\n",
        "grid=GridSearchCV(LogisticRegression(max_iter=500),param,n_jobs=-1)\n",
        "grid.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "# Get the predicted classes\n",
        "trainscore =  grid.score(x_train,y_train)\n",
        "testscore =  grid.score(x_test,y_test)\n",
        "\n",
        "print(\"train score: {}\".format(trainscore),'\\n')\n",
        "print(\"test score: {}\".format(testscore),'\\n')\n",
        "\n",
        "y_predlogi =  grid.predict(x_test)\n",
        "print(' f1 score: ',f1_score(y_test, y_predlogi,average='micro'),'\\n')\n",
        "print(confusion_matrix(y_test, y_predlogi))"
      ],
      "metadata": {
        "id": "J2I-1U5zpUSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(' precision score: ',precision_score(y_test, y_predlogi,average='micro'),'\\n')\n",
        "print(' recall score: ',recall_score(y_test, y_predlogi,average='micro'),'\\n')\n",
        "print(classification_report(y_test, y_predlogi))"
      ],
      "metadata": {
        "id": "Yr9c98_fp-gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My goal should be to find the best hyperparameters values to get the perfect prediction results from our model. But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced. Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training score is 95% and testing score 94%.\n",
        "\n",
        "For testing dataset,\n",
        "\n",
        "a.I got low(0) price precision 98% of and recall of 95% and f1-score of 97%.\n",
        "\n",
        "b.I got medium(1) price precision 89% of and recall of 94% and f1-score of 91%.\n",
        "\n",
        "c.I got High(2) price precision 91% of and recall of 90% and f1-score of 91%.\n",
        "\n",
        "d.I got very high(3) price precision 96% of and recall of 96% and f1-score of 96%."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - K_nearest neighbours(knn)"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import KNeighborsClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Setup arrays to store training and test accuracies\n",
        "neighbors = np.arange(1,100)\n",
        "train_accuracy =np.empty(len(neighbors))\n",
        "test_accuracy = np.empty(len(neighbors))\n",
        "\n",
        "for i,k in enumerate(neighbors):\n",
        "    # Setup a knn classifier with k neighbors\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "\n",
        "    # Fit the model\n",
        "    knn.fit(x_train, y_train)\n",
        "\n",
        "    # Compute accuracy on the training set\n",
        "    train_accuracy[i] = knn.score(x_train, y_train)\n",
        "\n",
        "    # Compute accuracy on the test set\n",
        "    test_accuracy[i] = knn.score(x_test, y_test)"
      ],
      "metadata": {
        "id": "yZl-CathqXJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate plot\n",
        "plt.title('k-NN Varying number of neighbors')\n",
        "plt.plot(neighbors, test_accuracy, label='Testing Accuracy')\n",
        "plt.plot(neighbors, train_accuracy, label='Training accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('Number of neighbors')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dsaw0iY2q2Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup a knn classifier with k neighbors\n",
        "knn = KNeighborsClassifier(n_neighbors=90)\n",
        "\n",
        "# Fit the model\n",
        "knn.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "dHzPQ249rpL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "# Get the predicted classes\n",
        "trainscore =  knn.score(x_train,y_train)\n",
        "testscore =  knn.score(x_test,y_test)\n",
        "\n",
        "print(\"train score: {}\".format(trainscore),'\\n')\n",
        "print(\"test score: {}\".format(testscore),'\\n')\n",
        "\n",
        "y_predlogi =  knn.predict(x_test)\n",
        "print(' f1 score: ',f1_score(y_test, y_predlogi,average='micro'),'\\n')\n",
        "print(confusion_matrix(y_test, y_predlogi))"
      ],
      "metadata": {
        "id": "P7Yw8IsTr-6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(' precision score: ',precision_score(y_test, y_predlogi,average='micro'),'\\n')\n",
        "print(' recall score: ',recall_score(y_test, y_predlogi,average='micro'),'\\n')\n",
        "print(classification_report(y_test, y_predlogi))"
      ],
      "metadata": {
        "id": "mPjPDySisMIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------- K_neigbours classifier -------------------------------------\n",
        "probabilityValues = knn.predict_proba(x)\n",
        "auc = roc_auc_score(y,probabilityValues,multi_class ='ovr')\n",
        "print(auc)"
      ],
      "metadata": {
        "id": "bg2yDiz_sOaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "-wNqPsWVuUSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.I used knn classifier to create the model. As I got not so good result.\n",
        "\n",
        "2.For training score is 56% and testing score 52%\n",
        "\n",
        "3.For testing dataset, i found precision of 52% and recall of 52% and f1-score of 52% . BUt, I am also interested to see the result for mobile price range result as\n",
        "\n",
        "a.I got low(0) price precision 68% of and recall of 67% and f1-score of 67%.\n",
        "\n",
        "b.I got medium(1) price precision 36% of and recall of 42% and f1-score of 39%.\n",
        "\n",
        "c.I got High(2) price precision 41% of and recall of 55% and f1-score of 47%.\n",
        "\n",
        "d.I got very high(3) price precision 79% of and recall of 46% and f1-score of 58%."
      ],
      "metadata": {
        "id": "_qC4sstqP4Y5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply naive_bayes_classifier\n",
        "NBmodel = GaussianNB()\n",
        "NBmodel.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainscore =  NBmodel.score(x_train,y_train)\n",
        "testscore =  NBmodel.score(x_test,y_test)\n",
        "\n",
        "print(\"train score: {}\".format(trainscore),'\\n')\n",
        "print(\"test score: {}\".format(testscore),'\\n')\n",
        "\n",
        "y_predNB =  NBmodel.predict(x_test)\n",
        "print(' f1 score: ',f1_score(y_test, y_predNB,average='micro'),'\\n')\n",
        "print(confusion_matrix(y_test, y_predNB))"
      ],
      "metadata": {
        "id": "5s3FFsjgslEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(' precision score: ',precision_score(y_test, y_predNB,average='micro'),'\\n')\n",
        "print(' recall score: ',recall_score(y_test, y_predNB,average='micro'),'\\n')\n",
        "print(classification_report(y_test, y_predNB))"
      ],
      "metadata": {
        "id": "j_7dF39Esxw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------- naive bayes -------------------------------------\n",
        "probabilityValues = NBmodel.predict_proba(x)\n",
        "auc = roc_auc_score(y,probabilityValues,multi_class ='ovr')\n",
        "print(auc)"
      ],
      "metadata": {
        "id": "6xEMh3_ls4F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.I used naive bayes algorithm to create the model. As I got not so good result.\n",
        "\n",
        "2.For training score is 81% and testing score 80%\n",
        "\n",
        "3.For testing dataset, i found precision of 80% and recall of 80% and f1-score of 80% . BUt, I am also interested to see the result for mobile price range result as\n",
        "\n",
        "a.I got low(0) price precision 95% of and recall of 88% and f1-score of 91%.\n",
        "\n",
        "b.I got medium(1) price precision 69% of and recall of 72% and f1-score of 71%.\n",
        "\n",
        "c.I got High(2) price precision 68% of and recall of 67% and f1-score of 68%.\n",
        "\n",
        "d.I got very high(3) price precision 87% of and recall of 90% and f1-score of 88%."
      ],
      "metadata": {
        "id": "3OYbfqjdQLy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - Support Vector Machine Classifier"
      ],
      "metadata": {
        "id": "4tmOmL0cuszY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying support vector machine classifier\n",
        "svcmodel = SVC(probability=True)\n",
        "svcmodel.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "0TXaX1Kvu5jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainscore =  svcmodel.score(x_train,y_train)\n",
        "testscore =  svcmodel.score(x_test,y_test)\n",
        "\n",
        "print(\"train score: {}\".format(trainscore),'\\n')\n",
        "print(\"test score: {} \".format(testscore),'\\n')\n",
        "\n",
        "y_predsvc =  svcmodel.predict(x_test)\n",
        "print(' f1 score: ',f1_score(y_test, y_predsvc,average='micro'),'\\n')\n",
        "print(confusion_matrix(y_test, y_predsvc))"
      ],
      "metadata": {
        "id": "zeM0-cniu-d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(' precision score: ',precision_score(y_test, y_predsvc,average='micro'),'\\n')\n",
        "print(' recall score: ',recall_score(y_test, y_predsvc,average='micro'),'\\n')\n",
        "print(classification_report(y_test, y_predsvc))"
      ],
      "metadata": {
        "id": "jTkImCL8vHAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------- support vector classification -------------------------------------\n",
        "probabilityValues = svcmodel.predict_proba(x)\n",
        "auc = roc_auc_score(y,probabilityValues,multi_class ='ovr')\n",
        "print(auc)"
      ],
      "metadata": {
        "id": "svq7L3HBvIx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "R0sMrvBBvTA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.I used support vector machine to create the model. As I got not so good result.\n",
        "\n",
        "2.For training score is 95% and testing score 83%\n",
        "\n",
        "3.For testing dataset, i found precision of 83% and recall of 83% and f1-score of 83% . BUt, I am also interested to see the result for mobile price range result as\n",
        "\n",
        "a.I got low(0) price precision 96% of and recall of 83% and f1-score of 89%.\n",
        "\n",
        "b.I got medium(1) price precision 73% of and recall of 81% and f1-score of 77%.\n",
        "\n",
        "c.I got High(2) price precision 74% of and recall of 81% and f1-score of 77%.\n",
        "\n",
        "d.I got very high(3) price precision 92% of and recall of 85% and f1-score of 88%."
      ],
      "metadata": {
        "id": "58nLXggXQbUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - Decision Tree"
      ],
      "metadata": {
        "id": "Qm_qTSPrvffD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Decision Tree\n",
        "DTmodel=  DecisionTreeClassifier(max_depth=6)\n",
        "DTmodel.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "BMIGycrNv81o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainscore =  DTmodel.score(x_train,y_train)\n",
        "testscore =  DTmodel.score(x_test,y_test)\n",
        "y_predDT =  DTmodel.predict(x_test)\n",
        "\n",
        "print(\"train score: {}\".format(trainscore),'\\n')\n",
        "print(\"test score: {} \".format(testscore),'\\n')\n",
        "\n",
        "print(' f1 score: ',f1_score(y_test, y_predDT,average='micro'),'\\n')\n",
        "print(confusion_matrix(y_test, y_predDT))"
      ],
      "metadata": {
        "id": "GfOmd7VswCAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(' precision score: ',precision_score(y_test, y_predDT,average='micro'),'\\n')\n",
        "print(' recall score: ',recall_score(y_test, y_predDT,average='micro'),'\\n')\n",
        "print(classification_report(y_test, y_predDT))"
      ],
      "metadata": {
        "id": "26D1GYjlwFeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------- Decision Tree -------------------------------------\n",
        "probabilityValues = DTmodel.predict_proba(x)\n",
        "auc = roc_auc_score(y,probabilityValues,multi_class ='ovr')\n",
        "print(auc)"
      ],
      "metadata": {
        "id": "f0gJXbXjwLXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AoC6L0KwZCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.I have used Decision Tree to create the model. As I got not so good result.\n",
        "\n",
        "2.For training score is 93% and testing score 85%\n",
        "\n",
        "3.For testing dataset, i found precision of 85% and recall of 85% and f1-score of 85% . BUt, I am also interested to see the result for mobile price range result as\n",
        "\n",
        "a.I got low(0) price precision 99% of and recall of 86% and f1-score of 92%.\n",
        "\n",
        "b.I got medium(1) price precision 74% of and recall of 87% and f1-score of 80%.\n",
        "\n",
        "c.I got High(2) price precision 79% of and recall of 77% and f1-score of 78%.\n",
        "\n",
        "d.I got very high(3) price precision 91% of and recall of 91% and f1-score of 91%."
      ],
      "metadata": {
        "id": "_rS3gyJRQm8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "ymY51vSGwZCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross validation\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "gridDT = GridSearchCV(DTmodel, param_grid = {'max_depth': (5, 30), 'max_leaf_nodes': (10, 100)}, scoring = 'accuracy', cv = 5, verbose = 24)\n",
        "gridDT.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "sP0hIs0dwZCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find best params value\n",
        "gridDT.best_params_"
      ],
      "metadata": {
        "id": "2A4gbtQzwo2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find best estimeter\n",
        "gridDT.best_estimator_"
      ],
      "metadata": {
        "id": "d-xrd9hMwp__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DTgmodel=DecisionTreeClassifier(max_depth=5, max_leaf_nodes=100)\n",
        "DTgmodel.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "NZftTrbrwwbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "# Get the predicted probabilities\n",
        "trainscore =  DTgmodel.score(x_train,y_train)\n",
        "testscore =  DTgmodel.score(x_test,y_test)\n",
        "print(\"train score: {}\".format(trainscore),'\\n')\n",
        "print(\"test score: {}\".format(testscore),'\\n')\n",
        "\n",
        "y_predgDT = DTgmodel.predict(x_test)\n",
        "print(' f1 score: ',f1_score(y_test, y_predgDT,average='micro'),'\\n')\n",
        "\n",
        "# Get the confusion matrix for both train and test\n",
        "print(confusion_matrix(y_test, y_predgDT))"
      ],
      "metadata": {
        "id": "ppA0Yb7Aw0Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "y_pred_test = DTgmodel.predict(x_test)\n",
        "y_pres_train = DTgmodel.predict(x_train)\n",
        "\n",
        "# Evaluation metrics for test\n",
        "print(' precision score: ',precision_score(y_test, y_predgDT,average='micro'),'\\n')\n",
        "print(' recall score: ',recall_score(y_test, y_predgDT,average='micro'),'\\n')\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "id": "wDxd-0Few9Uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------- Decision Tree -------------------------------------\n",
        "probabilityValues = DTmodel.predict_proba(x)\n",
        "auc = roc_auc_score(y,probabilityValues,multi_class ='ovr')\n",
        "print(auc)"
      ],
      "metadata": {
        "id": "qcpKLSZmw90o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "NFzqmcCNwZCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model. But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced. Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "T8CUCnNuwZCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "2ww5FlKqwZCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training score is 89% and testing score 84%.\n",
        "\n",
        "For testing dataset,\n",
        "\n",
        "a.I got low(0) price precision 98% of and recall of 86% and f1-score of 92%.\n",
        "\n",
        "b.I got medium(1) price precision 74% of and recall of 90% and f1-score of 81%.\n",
        "\n",
        "c.I got High(2) price precision 79% of and recall of 71% and f1-score of 75%.\n",
        "\n",
        "d.I got very high(3) price precision 88% of and recall of 91% and f1-score of 89%."
      ],
      "metadata": {
        "id": "Wuy6r9ZKwZCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - Random Forest Classifier"
      ],
      "metadata": {
        "id": "6vPKYdqwxPxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying random forest classifier\n",
        "RFmodel=  RandomForestClassifier(criterion='entropy', max_depth=9)\n",
        "RFmodel.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "3oGFZCGPxi0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainscore =  RFmodel.score(x_train,y_train)\n",
        "testscore =  RFmodel.score(x_test,y_test)\n",
        "y_predRF =  RFmodel.predict(x_test)\n",
        "\n",
        "print(\"train score: {}\".format(trainscore),'\\n')\n",
        "print(\"test score: {} \".format(testscore),'\\n')\n",
        "\n",
        "print(' f1 score: ',f1_score(y_test, y_predRF,average='micro'),'\\n')\n",
        "print(confusion_matrix(y_test, y_predRF))"
      ],
      "metadata": {
        "id": "QX4A1siVxqap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(' precision score: ',precision_score(y_test, y_predRF,average='micro'),'\\n')\n",
        "print(' recall score: ',recall_score(y_test, y_predRF,average='micro'),'\\n')\n",
        "print(classification_report(y_test, y_predRF))"
      ],
      "metadata": {
        "id": "rhoac2MyxvtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------- Random Forest -------------------------------------\n",
        "probabilityValues = RFmodel.predict_proba(x)\n",
        "auc = roc_auc_score(y,probabilityValues,multi_class ='ovr')\n",
        "print(auc)"
      ],
      "metadata": {
        "id": "dnwEWCaCxxkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "mkYn6XL2xaS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.I have used Random Forest classifier to create the model. As I got not so good result.\n",
        "\n",
        "2.For training score is 99% and testing score 86%\n",
        "\n",
        "3.For testing dataset, i found precision of 86% and recall of 86% and f1-score of 86% . BUt, I am also interested to see the result for mobile price range result as\n",
        "\n",
        "a.I got low(0) price precision 97% of and recall of 91% and f1-score of 94%.\n",
        "\n",
        "b.I got medium(1) price precision 76% of and recall of 86% and f1-score of 81%.\n",
        "\n",
        "c.I got High(2) price precision 80% of and recall of 75% and f1-score of 77%.\n",
        "\n",
        "d.I got very high(3) price precision 91% of and recall of 92% and f1-score of 91%."
      ],
      "metadata": {
        "id": "UNj1rL-tRHmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "Oz1Bo-faxaTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cross validation\n",
        "params = {'n_estimators':[10,50,100,200],\n",
        "          'max_depth':[10,20,30,40],\n",
        "          'min_samples_split':[2,4,6],\n",
        "         'max_features':['sqrt',4,'log2','auto'],\n",
        "         'max_leaf_nodes':[10, 20, 40]\n",
        "         }\n",
        "rf = RandomForestClassifier()\n",
        "rfgd = GridSearchCV(rf, params, scoring='accuracy', cv=3)\n",
        "rfgd.fit(x, y)"
      ],
      "metadata": {
        "id": "0qVQc9j3xaTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find best params value\n",
        "rfgd.best_params_"
      ],
      "metadata": {
        "id": "JQcPXXCTyBCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find best estimeter\n",
        "rfgd.best_estimator_"
      ],
      "metadata": {
        "id": "Q0w7JLq5zZUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find best score\n",
        "rfgd.best_score_"
      ],
      "metadata": {
        "id": "-mvHJelZzdEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfgd = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
        "                       criterion='gini', max_depth=30, max_features='log2',\n",
        "                       max_leaf_nodes=40, max_samples=None,\n",
        "                       min_impurity_decrease=0.0,\n",
        "                       min_samples_leaf=1, min_samples_split=4,\n",
        "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
        "                       n_jobs=None, oob_score=False, random_state=None,\n",
        "                       verbose=0, warm_start=False)\n",
        "rfgd.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "mtvbokS4zgQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "y_pred_test = rfgd.predict(x_test)\n",
        "y_pres_train = rfgd.predict(x_train)\n",
        "\n",
        "# Evaluation metrics for test\n",
        "trainscore =  RFmodel.score(x_train,y_train)\n",
        "testscore =  RFmodel.score(x_test,y_test)\n",
        "\n",
        "print(\"train score: {}\".format(trainscore),'\\n')\n",
        "print(\"test score: {} \".format(testscore),'\\n')\n",
        "print(' precision score: ',precision_score(y_test, y_pred_test,average='micro'),'\\n')\n",
        "print(' recall score: ',recall_score(y_test, y_pred_test,average='micro'),'\\n')\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "id": "s85MB5bLzk0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "FPyt5vzSxaTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model. But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced. Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "J6w3S9gdxaTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "3OEoA9XxxaTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training score is 99% and testing score 84%.\n",
        "\n",
        "For testing dataset,\n",
        "\n",
        "a.I got low(0) price precision 96% of and recall of 94% and f1-score of 95%.\n",
        "\n",
        "b.I got medium(1) price precision 76% of and recall of 82% and f1-score of 79%.\n",
        "\n",
        "c.I got High(2) price precision 77% of and recall of 75% and f1-score of 76%.\n",
        "\n",
        "d.I got very high(3) price precision 93% of and recall of 91% and f1-score of 92%."
      ],
      "metadata": {
        "id": "zrS_xK1zxaTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - XGBclassifier"
      ],
      "metadata": {
        "id": "YsFyQvCn0IwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(max_depth = 5, learning_rate = 0.1)\n",
        "xgb.fit(x_train, y_train)\n",
        "XGBClassifier(max_depth=5, objective='multi:softprob')"
      ],
      "metadata": {
        "id": "4rm8fmK40fi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "y_pred_train = xgb.predict(x_train)\n",
        "y_pred_test = xgb.predict(x_test)\n",
        "\n",
        "# Evaluation metrics for test\n",
        "print('Train_score:',accuracy_score(y_train,y_pred_train),'\\n')\n",
        "print('test score:',accuracy_score(y_test,y_pred_test),'\\n')\n",
        "\n",
        "print(' precision score: ',precision_score(y_test, y_pred_test,average='micro'),'\\n')\n",
        "print(' recall score: ',recall_score(y_test, y_pred_test,average='micro'),'\\n')\n",
        "\n",
        "score = classification_report(y_test, y_pred_test)\n",
        "print(score)"
      ],
      "metadata": {
        "id": "Y87fphCS0l8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "FhgDFOI40di8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.I used xgb classifier to create the model. As I got so good result.\n",
        "\n",
        "2.For training score is 100% and testing score 89%\n",
        "\n",
        "3.For testing dataset, i found precision of 89% and recall of 89% and f1-score of 89% . BUt, I am also interested to see the result for mobile price range result as\n",
        "\n",
        "a.I got low(0) price precision 98% of and recall of 89% and f1-score of 94%.\n",
        "\n",
        "b.I got medium(1) price precision 80% of and recall of 94% and f1-score of 87%.\n",
        "\n",
        "c.I got High(2) price precision 88% of and recall of 80% and f1-score of 84%.\n",
        "\n",
        "d.I got very high(3) price precision 91% of and recall of 94% and f1-score of 92%."
      ],
      "metadata": {
        "id": "D1dVmhpySHEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "CVzldJLD0dkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross validation\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "grid = GridSearchCV(xgb, param_grid={'n_estimators': (10, 200), 'learning_rate': [1, 0.5, 0.1, 0.01, 0.001], 'max_depth': (5, 10),\n",
        "                                     'gamma': [1.5, 1.8], 'subsample': [0.3, 0.5, 0.8]}, cv = 5, scoring = 'accuracy', verbose = 10)\n",
        "grid.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "PIyMt3SE0dkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find best params value\n",
        "grid.best_params_"
      ],
      "metadata": {
        "id": "zslfElqd2avv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find best estimeter\n",
        "grid.best_estimator_"
      ],
      "metadata": {
        "id": "4NJHK-bA2de1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#applying best estimeter value in xgboost\n",
        "model=XGBClassifier(gamma=1.5, max_depth=10, n_estimators=200,\n",
        "              objective='multi:softprob', subsample=0.5)\n",
        "model.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "OmRIS9dO2g1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "# Get the predicted probabilities\n",
        "trainscore =  model.score(x_train,y_train)\n",
        "testscore =  model.score(x_test,y_test)\n",
        "print(\"train score: {}\".format(trainscore),'\\n')\n",
        "print(\"test score: {}\".format(testscore),'\\n')\n",
        "\n",
        "y_predxgb = model.predict(x_test)\n",
        "print(' f1 score: ',f1_score(y_test, y_predxgb,average='micro'),'\\n')\n",
        "\n",
        "# Get the confusion matrix for both train and test\n",
        "print(confusion_matrix(y_test, y_predxgb))"
      ],
      "metadata": {
        "id": "NWKcQtmu2lly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics for train\n",
        "print(' precision score: ',precision_score(y_test, y_predxgb,average='micro'),'\\n')\n",
        "print(' recall score: ',recall_score(y_test, y_predxgb,average='micro'),'\\n')\n",
        "\n",
        "score = classification_report(y_train, y_pred_train)\n",
        "print(score)"
      ],
      "metadata": {
        "id": "XKBR3WXd2smq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------- XGBoosting -------------------------------------\n",
        "probabilityValues = RFmodel.predict_proba(x)\n",
        "auc = roc_auc_score(y,probabilityValues,multi_class ='ovr')\n",
        "print(auc)"
      ],
      "metadata": {
        "id": "ygCuir7d21kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "R7xSoiW70dkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model. But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced. Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "zLNsmpq00dkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "jKQmqlbe0dkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training score is 98% and testing score 89%.\n",
        "\n",
        "For testing dataset,\n",
        "\n",
        "a.I got low(0) price precision 100% of and recall of 100% and f1-score of 100%.\n",
        "\n",
        "b.I got medium(1) price precision 100% of and recall of 100% and f1-score of 81%.\n",
        "\n",
        "c.I got High(2) price precision 100% of and recall of 100% and f1-score of 100%.\n",
        "\n",
        "d.I got very high(3) price precision 100% of and recall of 100% and f1-score of 100%."
      ],
      "metadata": {
        "id": "xf5vF-7X0dkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When considering evaluation metrics for classification models in the context of a positive business impact, the choice depends on the specific goals and requirements of the business. However, some commonly used evaluation metrics that can provide insights for a positive business impact are:\n",
        "\n",
        "* Accuracy\n",
        "* Precision\n",
        "* Recall\n",
        "* F1-score\n",
        "* AUC-ROC\n",
        "\n",
        "The choice of evaluation metrics depends on the specific business context and the importance of different types of classification errors. It is recommended to consider multiple evaluation metrics to gain a comprehensive understanding of the model's performance and its potential impact on business outcomes."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have choosen XGBoost model and logistic regression which is hyperparameter optimized. first of all I need accuracy for the mobile price range prediction. Thus, for greater accuracy we used kernel SVM, Random Forest, XgBoost. So, I tried both logistic and XGBoost. Here is their ealuation metrics and I would like to compare."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost (eXtreme Gradient Boosting) is a powerful and widely used gradient boosting framework for machine learning tasks, particularly in tabular data settings. It is known for its efficiency, scalability, and high performance in various domains, including classification, regression, and ranking problems.\n",
        "\n",
        "XGBoost builds an ensemble of weak prediction models, typically decision trees, in a sequential manner. Each new model is trained to correct the errors made by the previous models, gradually improving the overall predictive performance. The boosting process focuses on minimizing a predefined loss function by optimizing the model parameters, such as tree structure and leaf weights, through gradient descent.\n",
        "\n",
        "The XGBoost model offers several key features and advantages:\n",
        "\n",
        "* Regularization Techniques: XGBoost provides built-in regularization techniques to prevent overfitting, such as L1 and L2 regularization, which help control the complexity of the model and improve generalization.\n",
        "\n",
        "* Handling Missing Values: XGBoost has the capability to handle missing values in the dataset. It automatically learns how to deal with missing values during the tree construction process, without requiring explicit data imputation.\n",
        "\n",
        "* Feature Importance: XGBoost provides a built-in mechanism to calculate feature importance scores. These scores indicate the relative importance of each feature in contributing to the model's overall performance. The feature importance can help identify the most influential features and gain insights into the underlying data relationships.\n",
        "\n",
        "To explore feature importance using a model explainability tool, one popular option is the \"XGBoost.plot_importance\" function from the XGBoost library in Python. This function generates a bar plot showing the importance scores of the features based on the XGBoost model."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import pickle\n",
        "filename = 'finalized_mprp_model.pickle'\n",
        "pickle.dump(xgb, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "with open('finalized_mprp_model.pickle', 'rb') as file:\n",
        "    model = pickle.load(file)\n",
        "\n",
        "unseen_data = x[5]\n",
        "predictions = xgb.predict([unseen_data])\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this article, I looked at classification. Classifiers represent the intersection of advanced machine theory and practical application. These algorithms are more than just a sorting mechanism for organising unlabeled data instances into distinct groupings. Classifiers include a unique set of dynamic rules that include an interpretation mechanism for dealing with ambiguous or unknown values, all of which are suited to the kind of inputs being analysed. Most classifiers also utilise probability estimates, which enable end-users to adjust data categorization using utility functions.\n",
        "\n",
        "1. From EDA we can see that here are mobile phones in 4 price ranges. The number of elements is almost similar.\n",
        "\n",
        "2. Half the devices have Bluetooth, and half don’t\n",
        "\n",
        "3. There is a gradual increase in battery as the price range increases\n",
        "\n",
        "4. Ram has continuous increase with price range while moving from Low cost to Very high cost.\n",
        "\n",
        "5. Costly phones are lighter.\n",
        "\n",
        "6. RAM, battery power, pixels played more significant role in deciding the price range of mobile phone.\n",
        "\n",
        "7. Form all the above experiments we can conclude that XGboosting and linear regression with using hyperparameters we got the best results."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}